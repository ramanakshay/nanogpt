defaults:
    - model: gpt2-xl
    - _self_

##########################################################################
# effective batchsize
# 32 batch size * 1024 block size * 1 gradaccum * 1 GPU(s) = 32768 tokens/iter

# shakespeare has 301,966 tokens, so 1 epoch ~= 10 iters
# running about 2 epochs for finetuning
##########################################################################

system:
    device: cuda
    backend: 'nccl'
    dtype: bfloat16
    to_compile: False

data:
    data_dir: /scratch/ar8692/nanogpt/src/data/datasets/shakespeare/
    batch_size: 1 # choose batch size that fills GPU memory
    block_size: 1024

algorithm:
    max_iters: 20
    grad_accum: 32

    # optimizer
    lr: 3e-5
    weight_decay: 1e-1
    betas: [0.9, 0.95]
    eps: 1e-8

    # norm
    grad_clip: 1.0

    # scheduler
    decay_lr: True # whether to decay the learning rate
    warmup_iters: 10 # how many steps to warm up for
    lr_decay_iters: ${algorithm.max_iters} # should be ~= max_iters per Chinchilla
    min_lr: 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

    # eval
    eval_interval: 1000
    eval_iters: 200