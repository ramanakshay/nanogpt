defaults:
    - _self_

##########################################################################
# these make the total batch size be ~0.5M
# 64 batch size * 1024 block size * 1 grad accum * 8 GPU(s) = 524288 tokens

# max_iterse goal to process 300B tokens
# 524288 tokens/iter * 600000 iter= 300B tokens
##########################################################################

system:
    device: cuda
    backend: 'nccl'
    dtype: float16
    to_compile: True

model:
    from_pretrained: False
    model_name: None
    block_size: 1024
    vocab_size: 50304
    n_layer: 12
    n_head: 12
    n_embd: 768
    dropout: 0.0
    bias: True

data:
    data_dir: /scratch/ar8692/nanogpt/src/data/datasets/openwebtext/
    batch_size: 64 # choose batch size that fills GPU memory
    block_size: 1024

algorithm:
    max_iters: 600000
    grad_accum: 1

    # optimizer
    lr: 6e-4
    weight_decay: 1e-1
    betas: [0.9, 0.95]
    eps: 1e-8

    # norm
    grad_clip: 1.0

    # scheduler
    decay_lr: True # whether to decay the learning rate
    warmup_iters: 10 # how many steps to warm up for
    lr_decay_iters: ${algorithm.max_iters} # should be ~= max_iters per Chinchilla
    min_lr: 6e-5 # minimum learning rate, should be ~= learning_rate/10 per Chinchilla

    # eval
    eval_interval: 1000
    eval_iters: 200